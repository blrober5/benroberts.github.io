{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Light GBM Model Using Bayesian HyperParameter Tuning Plus Model Evaluation and Interpretation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Read in Libraries\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot\n",
    "import sklearn\n",
    "import sklearn.preprocessing\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import f1_score\n",
    "import lightgbm as lgb\n",
    "from hyperopt import fmin, tpe, hp, STATUS_OK, Trials\n",
    "import hyperopt.pyll\n",
    "from hyperopt.pyll import scope\n",
    "import sklearn.inspection\n",
    "import shap\n",
    "\n",
    "##Set Working Directory\n",
    "os.chdir()\n",
    "\n",
    "##Read in Training Data\n",
    "Train = pd.read_csv('train.csv', delimiter=',', index_col=None)\n",
    "\n",
    "##Read in Validation Data\n",
    "Test = pd.read_csv('test.csv', delimiter=',', index_col=None)\n",
    "\n",
    "##View Data\n",
    "print(Train.shape)\n",
    "print(Train['target'].value_counts())\n",
    "Train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preprocessing Function "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Impute, Oversample, Encode Categorical Variables and Create Model Inputs for Data\n",
    "\n",
    "#Returns Final Training and Validation Sets along with Train and Test matrices to input into the model\n",
    "\n",
    "def preprocessing(Train, Test, target):\n",
    "    \n",
    "    #List of Categorical Variables\n",
    "    cat = Train.select_dtypes(include=['object', 'int64']).columns.tolist()\n",
    "    \n",
    "    #List of Continuous Variables\n",
    "    cont = Train.select_dtypes(include=['float64']).columns.tolist()\n",
    "\n",
    "    ##Imputing Data\n",
    "    #Categorical - Set to 'Missing'/-1\n",
    "    Train = Train[cat].fillna(-1)\n",
    "    \n",
    "    #Numerical - Impute Median of Column\n",
    "    Train = Train[cont].fillna(Train.median())\n",
    "    \n",
    "    ##Over Sampling\n",
    "    Train_1 = Train[Train[target]==1]\n",
    "    #Change to Fit Desired Proportion of 1s to 0s\n",
    "    Train_over = Train.append([Train_1]*10, ignore_index=True)\n",
    "    print(Train_over[target].value_counts())\n",
    "    print(Train_over.shape)\n",
    "\n",
    "    ##Label Encoding Ordinal Variables\n",
    "    ordinal = []\n",
    "    #Training\n",
    "    for feat in ordinal:\n",
    "        le=sklearn.preprocessing.LabelEncoder()\n",
    "    Train_over[feat] = le.fit_transform(Train_over[feat])\n",
    "    #Validation\n",
    "    for feat in ordinal:\n",
    "        le=sklearn.preprocessing.LabelEncoder()\n",
    "        Test[feat] = le.fit_transform(Test[feat])\n",
    "\n",
    "    ##One Hot Encoding\n",
    "    #Training\n",
    "    Train_over=pd.get_dummies(Train_over, drop_first=True)\n",
    "    #Validation\n",
    "    Test=pd.get_dummies(Test, drop_first=True)\n",
    "\n",
    "    ##Converting Data to Type Int for Model\n",
    "    Train_ova[Train_ova.select_dtypes(['uint8']).columns]=Train_ova.select_dtypes(['uint8']).apply(lambda x: x.astype(float))\n",
    "    Test[Test.select_dtypes(['uint8']).columns]=Test.select_dtypes(['uint8']).apply(lambda x: x.astype(float))\n",
    "    \n",
    "    ##Creating Matrices\n",
    "    #Training Inputs\n",
    "    X_train=Train_over.drop(columns=[target])\n",
    "    #Validation Inputs\n",
    "    X_test=Test.drop(columns=[target])\n",
    "    #Training Target\n",
    "    y_train=Train_over[target]\n",
    "    #Validation Target\n",
    "    y_test=Test[target]\n",
    "    \n",
    "    #Cleaning Column Names\n",
    "    X_train.columns=[\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_train.columns]\n",
    "    X_test.columns=[\"\".join (c if c.isalnum() else \"_\" for c in str(x)) for x in X_test.columns]\n",
    "\n",
    "    ##Output\n",
    "    return Train_over, Test, X_train, X_test, y_train, y_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building Light GBM Model Using Bayesian HyperParameter Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###HyperParameter Tuning Function\n",
    "def hyperopt(X_train, X_test, y_train, y_test, param_space, num_eval):\n",
    "    \n",
    "    ##Setting HyperParamter Grid\n",
    "    param_hyperopt={\n",
    "        'learning rate': hp.loguniform('learning_rate', np.log(0.01), np.log(1)),\n",
    "        'max_depth': scope.int(hp.quniform('max_depth', 3, 15, 1)),\n",
    "        'n_estimators': scope.int(hp.quniform('n_estimators', 5, 100, 1)),\n",
    "        'num_leaves': scope.int(hp.quniform('num_leaves', 5, 50, 1)),\n",
    "        'colsample_bytree': hp.uniform('colsample_bytree', 0.6, 1.0),\n",
    "        'bagging_fraction': hp.uniform('bagging_fraction', 0.6, 1.0),\n",
    "        'boosting_type': 'gbdt',\n",
    "    }\n",
    "\n",
    "\n",
    "    ##Defining Objective Function for Tuning\n",
    "    def objective_function(params):\n",
    "\n",
    "        #Evaluating LightGBM Classification Model on Tuning Parameters\n",
    "        clf=lgb.LGBMClassifier(**params)\n",
    "\n",
    "        evaluation = [(X_train, y_train), (X_test, y_test)]\n",
    "            \n",
    "        #Training Model\n",
    "        clf.fit(X_train, y_train,\n",
    "                eval_set=evaluation, eval_metric='auc',\n",
    "                early_stopping_rounds=10, verbose=False)\n",
    "\n",
    "        #Score Model on Validation to Obtain Predicted Probabilities\n",
    "        preds=clf.predict_proba(X_test)\n",
    "        preds=preds[:,1]\n",
    "\n",
    "        #Adjusting Intercept of Predictions to Account for Oversampling Bias\n",
    "        #Change to fit Target Proportion in Training and Oversampled Training\n",
    "        newpreds=(preds * 0.8 * 0.02)/((1-preds) * 0.2 * 0.98 + preds * 0.8 * 0.02)\n",
    "\n",
    "        #Evaluate Model and Adjust Hyperparamters to Maximize AUC\n",
    "        auc=roc_auc_score(y_test, newpreds)\n",
    "\n",
    "        print('Score:', auc)\n",
    "        return {'loss': -auc, 'status': STATUS_OK}\n",
    "\n",
    "    trials=Trials()\n",
    "\n",
    "    #Parameter Tuning\n",
    "    best_param=fmin(objective_function,\n",
    "                        param_space,\n",
    "                        algo=tpe.suggest,\n",
    "                        max_evals=num_eval,\n",
    "                        trials=trials,\n",
    "                        rstate=np.random.RandomState(1))\n",
    "\n",
    "    return best_param\n",
    "\n",
    "##Running function to get best HyperParamters\n",
    "best=hyperopt(X_train, X_test, y_train, y_test, param_hyperopt, 20)\n",
    "\n",
    "best_param_values=[x for x in best.values()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Function to Build Model on Identified Best HyperParameters and Evaluate Model Performance\n",
    "def evaluate(X_train, X_test, y_train, y_test, best_param_values):\n",
    "    \n",
    "    #Identify Best Parameters\n",
    "    clf_best = lgb.LGBMClassifier(learning_rate = best_param_values[0],\n",
    "                                 max_depth = best_param_values[1],\n",
    "                                 n_estimators = best_param_values[2],\n",
    "                                 num_leaves = best_param_values[3],\n",
    "                                 colsample_bytree = best_param_values[4],\n",
    "                                 bagging_fraction = best_param_values[5],\n",
    "                                 boosting_type = 'gbdt')\n",
    "    #Build Model on Parameters\n",
    "    clf_best.fit(X_train, y_train)\n",
    "    \n",
    "    #Score Test Data\n",
    "    preds=clf_best.predict_proba(X_test)\n",
    "    preds=preds[:,1]\n",
    "    #Adjust Predictions\n",
    "    #Change to fit Target Proportion in Training and Oversampled Training\n",
    "    newpreds=(preds * 0.8 * 0.02)/((1-preds) * 0.2 * 0.98 + preds * 0.8 * 0.02)\n",
    "    \n",
    "    #AUC on Validation\n",
    "    auc=roc_auc_score(y_test, newpreds)\n",
    "    \n",
    "    #Track PPV and TPR Across Different Cutoffs\n",
    "    cutoff=[]\n",
    "    PPV=[]\n",
    "    TPR=[]\n",
    "    \n",
    "    for i in np.arange(0, 1, 0.01):\n",
    "        #Classifying Observations\n",
    "        event=np.where(newpreds > i, 1, 0)\n",
    "        #Positive Predicted Values\n",
    "        ppv=precision_score(y_test, event, average='binary')\n",
    "        #Confusion Matrix\n",
    "        cm=confusion_matrix(y_test, event)\n",
    "        tp=cm[1][1]\n",
    "        fn=cm[1][0]\n",
    "        fp=cm[0][1]\n",
    "        tn=cm[0][0]\n",
    "        #True Positive Rate\n",
    "        tpr=tp/(tp+fn)\n",
    "        #Appending Lists\n",
    "        cutoff.append(i)\n",
    "        PPV.append(ppv)\n",
    "        TPR.append(TPR)\n",
    "     \n",
    "    #DataFrame of Cutoff Stats at Each Cutoff\n",
    "    cutoff_stats = pd.DataFrame(data={'cutoff': cutoff, 'PPV': PPV, 'TPR': TPR})\n",
    "    \n",
    "    return cutoff_stats\n",
    "    \n",
    "evaluation = evaluate(X_train, X_test, y_train, y_test, best_param_values)\n",
    "\n",
    "evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Plot Model Stats (TPR and PPV) Across Different Cutoffs\n",
    "pyplot.plot(cutoff, PPV, color='red')\n",
    "pyplot.plot(cutoff, TPR, color='blue')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Confusion Matrix for Chosen Cutoff\n",
    "cm = confusion_matrix(y_test, newpreds > #fill-in with chosen cutoff)\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Interpretation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Feature Importance Plot\n",
    "lgb.plot_importance(clf_best, max_num_features=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###Change Data Type to Integer\n",
    "X_train=X_train.apply(lambda x: x.astype(int))\n",
    "X_test=X_test.apply(lambda x: x.astype(int))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###SHAP Summary Plot\n",
    "#SHAP Value (impact on model output) for different levels of key variables\n",
    "shap_value=shap.TreeExplainer(clf_best).shap_values(X_train)\n",
    "shap_value=np.array(shap_value[1])\n",
    "shap.summary_plot(shap_value, X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###SHAP Explainer, Individual Observation\n",
    "#For each individual observation, how each variable contributed to the final predicted probability\n",
    "shap.initjs()\n",
    "explainer=shap.SamplingExplainer(lambda x: clf_best.predict_proba(x)[:,1], data=X_train)\n",
    "\n",
    "#Apply Explainer to Observation 100\n",
    "shap_values=explainer.shap_values(X_train.loc[100,:])\n",
    "\n",
    "#Plot of SHAP Values for Individual Prediction\n",
    "shap.force_plot(explainer.expected_value, shap_values, features=X_train.loc[100,:],\n",
    "               feature_names=X_train.columns.tolist())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
