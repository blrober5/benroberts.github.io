{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m21LEth2j1tx"
   },
   "source": [
    "# Machine Learning Translation: English to French Using RNNs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WEFBG8qqj_la"
   },
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Q7lAeXIr868-"
   },
   "outputs": [],
   "source": [
    "# Importing NLTK and Libraries\n",
    "import nltk\n",
    "import string\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras.models import Model\n",
    "from keras.layers import GRU, Input, Dense, TimeDistributed, Activation, RepeatVector, Bidirectional\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "yxB0ZnEA881t",
    "outputId": "68f54c4b-99ab-4fd0-fdfe-ae2c5dfa3573"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['small_vocab_en.txt', 'small_vocab_fr.txt']"
      ]
     },
     "execution_count": 226,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Importing Corpus \n",
    "from nltk.corpus import PlaintextCorpusReader\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "corpus_root = '/content/drive/My Drive/en-fr'\n",
    "wordlists = PlaintextCorpusReader(corpus_root,'.*')\n",
    "wordlists.fileids()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ydYi_D5I9JSu"
   },
   "outputs": [],
   "source": [
    "# Assigning English and French Text\n",
    "en_text=wordlists.raw('small_vocab_en.txt')\n",
    "fr_text=wordlists.raw('small_vocab_fr.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yuWylTQO9GBu"
   },
   "outputs": [],
   "source": [
    "# Splitting Texts into Individual Sentences\n",
    "en_text = en_text.split('\\n')\n",
    "fr_text = fr_text.split('\\n')\n",
    "#print(en_text[0:3])\n",
    "#print(sp_text[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "1HPCyzUG92tQ"
   },
   "outputs": [],
   "source": [
    "# Splitting into list of lists of words\n",
    "en_text = [sent.split(' ') for sent in en_text]\n",
    "fr_text = [sent.split(' ') for sent in fr_text]\n",
    "\n",
    "# Removing Punctuation\n",
    "table = str.maketrans('','', string.punctuation)\n",
    "en_text = [[word.translate(table) for word in sent] for sent in en_text]\n",
    "fr_text = [[word.translate(table) for word in sent] for sent in fr_text]\n",
    "\n",
    "#print(en_text[0:3])\n",
    "#print('\\n')\n",
    "#print(sp_text[0:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "MH4qeCX395r-",
    "outputId": "6561b305-293b-4f9b-d18d-06810471ca91"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(English) Mean sentence length:  17\n",
      "(Spanish) Mean sentence length:  23\n",
      "(English) Vocabulary size:  200\n",
      "(Spanish) Vocabulary size:  346\n"
     ]
    }
   ],
   "source": [
    "# Compute length of English sentences\n",
    "en_sent_lengths = [len(en_sent) for en_sent in en_text]\n",
    "# Compute the max English sentence length\n",
    "en_len = int(round(np.max(en_sent_lengths)))\n",
    "print('(English) Mean sentence length: ', en_len)\n",
    "\n",
    "# Compute length of French sentences\n",
    "fr_sent_lengths = [len(fr_sent) for fr_sent in fr_text]\n",
    "# Compute the max French sentence length\n",
    "fr_len = int(round(np.max(fr_sent_lengths)))\n",
    "print('(Spanish) Mean sentence length: ', fr_len)\n",
    "\n",
    "# Compute length of English vocabulary\n",
    "en_words = []\n",
    "for sent in en_text:\n",
    "  # Populate all_words with all the words in sentences\n",
    "  en_words.extend(sent)\n",
    "# Compute the length of the set containing all_words\n",
    "en_vocab = len(set(en_words))\n",
    "print(\"(English) Vocabulary size: \", en_vocab)\n",
    "\n",
    "# Compute length of French vocabulary\n",
    "fr_words = []\n",
    "for sent in fr_text:\n",
    "  # Populate all_words with all the words in sentences\n",
    "  fr_words.extend(sent)\n",
    "# Compute the length of the set containing all_words\n",
    "fr_vocab = len(set(fr_words))\n",
    "print(\"(Spanish) Vocabulary size: \", fr_vocab)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "evU4gIdJ-B8h"
   },
   "outputs": [],
   "source": [
    "#Tokenizing Texts\n",
    "en_tok = Tokenizer()\n",
    "en_tok.fit_on_texts(en_text)\n",
    "\n",
    "fr_tok = Tokenizer()\n",
    "fr_tok.fit_on_texts(fr_text)\n",
    "\n",
    "# Combine text in each list into a full sentences\n",
    "en_sentences = [' '.join(sent) for sent in en_text]\n",
    "fr_sentences = [' '.join(sent) for sent in fr_text]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "t47wqp-m-0NQ"
   },
   "outputs": [],
   "source": [
    "# Convert Each English Sentence into a Sequence Vector the Length of the Longest French Sentence\n",
    "def sents2seqs(sentences, pad_type='post'):\n",
    "    # Each Word in the Sentence is Converted to a Number Representing its Rank in the Tokenized Vocabulary     \n",
    "    encoded_text = en_tok.texts_to_sequences(sentences)\n",
    "    # Pad Each Sentence with 0s so that Each Vector is the Same Length\n",
    "    preproc_text = pad_sequences(encoded_text, padding=pad_type, truncating='post', maxlen=fr_len)\n",
    "    return preproc_text\n",
    "    \n",
    "en_x = sents2seqs(en_sentences)\n",
    "#en_x=en_x.reshape(*en_x.shape, 1)\n",
    "#print(en_x.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JyITUHgWB-q5"
   },
   "outputs": [],
   "source": [
    "# Convert Each French Sentence into a Sequence Vector the Length of the Longest French Sentence\n",
    "def sents2seqs_fr(sentences, pad_type='post'):     \n",
    "    encoded_text = fr_tok.texts_to_sequences(sentences)\n",
    "    preproc_text = pad_sequences(encoded_text, padding=pad_type, truncating='post', maxlen=fr_len)\n",
    "    return preproc_text\n",
    "\n",
    "fr_y = sents2seqs_fr(fr_sentences)\n",
    "\n",
    "# Reshape French and English Vectors to Input into the Model\n",
    "fr_y=fr_y.reshape(*fr_y.shape, 1)\n",
    "en_x=en_x.reshape((-1, fr_y.shape[-2]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9EGOyI2snPQK"
   },
   "source": [
    "## Defining and Training the Embedded RNN Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 391
    },
    "colab_type": "code",
    "id": "9ER7PKjCS0H7",
    "outputId": "3097437c-327c-4d0d-8f65-1ec22c788dfb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 110288 samples, validate on 27573 samples\n",
      "Epoch 1/10\n",
      "110288/110288 [==============================] - 58s 528us/step - loss: 3.5736 - acc: 0.4511 - val_loss: 2.7546 - val_acc: 0.4641\n",
      "Epoch 2/10\n",
      "110288/110288 [==============================] - 55s 497us/step - loss: 2.5123 - acc: 0.4808 - val_loss: 2.2108 - val_acc: 0.5468\n",
      "Epoch 3/10\n",
      "110288/110288 [==============================] - 55s 496us/step - loss: 1.8630 - acc: 0.5940 - val_loss: 1.5829 - val_acc: 0.6305\n",
      "Epoch 4/10\n",
      "110288/110288 [==============================] - 55s 497us/step - loss: 1.4191 - acc: 0.6566 - val_loss: 1.2644 - val_acc: 0.6917\n",
      "Epoch 5/10\n",
      "110288/110288 [==============================] - 55s 496us/step - loss: 1.1611 - acc: 0.7186 - val_loss: 1.0666 - val_acc: 0.7408\n",
      "Epoch 6/10\n",
      "110288/110288 [==============================] - 55s 502us/step - loss: 1.0012 - acc: 0.7526 - val_loss: 0.9357 - val_acc: 0.7662\n",
      "Epoch 7/10\n",
      "110288/110288 [==============================] - 55s 500us/step - loss: 0.8844 - acc: 0.7767 - val_loss: 0.8309 - val_acc: 0.7886\n",
      "Epoch 8/10\n",
      "110288/110288 [==============================] - 55s 497us/step - loss: 0.7891 - acc: 0.7959 - val_loss: 0.7454 - val_acc: 0.8042\n",
      "Epoch 9/10\n",
      "110288/110288 [==============================] - 55s 497us/step - loss: 0.7119 - acc: 0.8109 - val_loss: 0.6786 - val_acc: 0.8184\n",
      "Epoch 10/10\n",
      "110288/110288 [==============================] - 55s 496us/step - loss: 0.6537 - acc: 0.8225 - val_loss: 0.6325 - val_acc: 0.8254\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f4673ce7160>"
      ]
     },
     "execution_count": 234,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Model Function\n",
    "def embed_model(input_shape, output_sequence_length, english_vocab_size, french_vocab_size):\n",
    "    learning_rate = 1e-3\n",
    "\n",
    "    # Setting Up Layers\n",
    "    rnn = GRU(64, return_sequences=True, activation=\"tanh\")\n",
    "    embedding = Embedding(french_vocab_size, 64, input_length=input_shape[1]) \n",
    "    logits = TimeDistributed(Dense(french_vocab_size+1, activation=\"softmax\"))\n",
    "    \n",
    "    model = Sequential()\n",
    "    model.add(embedding)\n",
    "    model.add(rnn)\n",
    "    model.add(logits)\n",
    "    \n",
    "    # Loss Function\n",
    "    model.compile(loss=sparse_categorical_crossentropy,\n",
    "                  optimizer=Adam(learning_rate),\n",
    "                  metrics=['accuracy'])\n",
    "    \n",
    "    return model\n",
    "\n",
    "\n",
    "# Training and Evaluating Model\n",
    "embedded_model = embed_model(\n",
    "    en_x.shape,\n",
    "    fr_len,\n",
    "    en_vocab,\n",
    "    fr_vocab)\n",
    "embedded_model.fit(en_x, fr_y, batch_size=1024, epochs=10, validation_split=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OLbdurFNotb0"
   },
   "source": [
    "## Translating an English Sentence to French"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PcN2yvSRLdGm",
    "outputId": "70e79ad1-51f1-4cec-e204-ced7c4331045"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "il est est chaud en <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD> <PAD>\n"
     ]
    }
   ],
   "source": [
    "def translate(sent):\n",
    "    #English Sentence\n",
    "    sentence = sent\n",
    "    #Converting to Sequence Vector\n",
    "    en_seq = sents2seqs(sentence)[0]\n",
    "    en_seq=en_seq.reshape((-1, fr_y.shape[-2]))\n",
    "    #Predicting French Sequence Vector\n",
    "    logits = embedded_model.predict(en_seq)[0]\n",
    "\n",
    "    # Creating Index of Words in Vocabulary to Convert Predicted Sequences to French Words\n",
    "    index_to_words = {id: word for word, id in fr_tok.word_index.items()}\n",
    "    index_to_words[0] = '<PAD>'\n",
    "    #Return French Translation\n",
    "    return ' '.join([index_to_words[prediction] for prediction in np.argmax(logits, 1)])\n",
    "\n",
    "print(translate(['it is hot today']))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Machine Learning Translation",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
